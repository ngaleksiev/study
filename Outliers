### matplotlib

import matplotlib.pyplot as plt

# subplots
plt.figure(figsize=(15,10))
plt.subplot(1,3,1)
plt.scatter(df[1],df[2])
plt.title('Title1')
plt.figure(figsize=(15,10))
plt.subplot(1,3,2)
plt.scatter(df[1],df[2])
plt.title('Title2')
plt.xlabel()
plt.ylabel()
plt.title()
plt.legend()
plt.show()


### sns
import seaborn as sns

plt.figure(figsize=(10,5))
ax = plt.subplot(1,1,1)
sns.heatmap(df[cols].corr(), ax=ax, cmap=plt.cm.RdBu, linewidths=0.1, vmax=1.0, square=True, linecolor='white', annot=True)    # correl heatmap w/ sns
plt.show()

fig, ax = plt.subplots(figsize=(10,5))
df[col1].hist(alpha=0.6, label='sp1', ax=ax)
df[col2].hist(alpha=0.6, label='sp2', ax=ax)
plt.legend()
plt.show()


### k-means clustering algorithm
## https://github.com/huseinzol05/Stock-Prediction-Models/blob/master/misc/outliers.ipynb
# number of clusters (k) needs to be specified a-priori
# use the elbow method (graphic) to estimate optimal number of clusters (k)

from sklean.cluster import KMeans
data = df.iloc[:,1:].dropna().values
kmeans = [KMeans(n_clusters=i).fit(data) for i in range(1,20)]
scores = [kmeans[i].score(data) for i in range(1,len(kmeans))]
fig,ax = plt.subplots(figsize=(10,5))
ax.plot(range(1,20),scores)
plt.xlabel('clusters')
plt.ylabel('scores')
plt.title('elbow curve')
plt.show()

km = KMeans(n_clusters=10)
X = df[[col1,col2,col2]].dropna()
X = X.reset_index(drop=True)
km.fit(X)
km.predict(X)
labels = km.labels_
